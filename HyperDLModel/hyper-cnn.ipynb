{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keras_tuner","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import Input, Dense, Embedding, Flatten, Conv2D, MaxPool2D, Bidirectional, LSTM, GRU, concatenate, GlobalMaxPooling1D, GlobalAveragePooling1D, SpatialDropout1D\nfrom keras.layers import Reshape, Flatten, Dropout, Concatenate\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Model, Sequential\nfrom keras import backend as K\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop=EarlyStopping(monitor='loss', patience=10)#, verbose=1)\nfrom sklearn.preprocessing import LabelEncoder\n# keras Tuner\nimport keras_tuner as kt\n# HP\nfrom keras_tuner import Hyperband\n\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score\nimport nltk\nimport string \nfrom tensorflow.keras.layers import SimpleRNN\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nlb_enc = LabelEncoder()\nfrom keras_tuner import RandomSearch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train = pd.read_excel('/kaggle/input/spam-review-detection/data_train.xlsx')\ndata_dev = pd.read_excel('/kaggle/input/spam-review-detection/data_dev.xlsx')\ndata_test = pd.read_excel('/kaggle/input/spam-review-detection/data_test.xlsx')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train=data_train['Label']\n\ny_dev=data_dev['Label']\n\ny_test=data_test['Label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer() \ntokenizer.fit_on_texts(data_train['transformed_text'])\ntokenizer.fit_on_texts(data_dev['transformed_text'])\ntokenizer.fit_on_texts(data_test['transformed_text'])\n\ntext_to_sequence_train = tokenizer.texts_to_sequences(data_train['transformed_text']) \ntext_to_sequence_dev = tokenizer.texts_to_sequences(data_dev['transformed_text']) \ntext_to_sequence_test = tokenizer.texts_to_sequences(data_test['transformed_text']) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length_sequence_train = max([len(i) for i in text_to_sequence_train])\n \npadded_train = pad_sequences(text_to_sequence_train, maxlen=max_length_sequence_train, \n                                    padding = \"pre\") \nmax_length_sequence_train\npadded_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_dev = pad_sequences(text_to_sequence_dev, maxlen=max_length_sequence_train, \n                                    padding = \"pre\") \nlen(padded_dev)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_test = pad_sequences(text_to_sequence_test, maxlen=max_length_sequence_train, \n                                    padding = \"pre\") \nlen(padded_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VOC_SIZE = len(tokenizer.word_index)+1 # 18359\n\ndef build_model(hp):\n    model = Sequential()\n    model.add(Embedding(VOC_SIZE, hp.Int('embedding_dim',  min_value=200, max_value=400, step=50), input_length=max_length_sequence_train))\n    \n    for i in range(hp.Int('num_layers', 1, 5)):\n        model.add(GRU(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32), \n                       activation='tanh', recurrent_activation='sigmoid', \n                       return_sequences=(i < (hp.Int('num_layers', 1, 3) - 1))))\n        model.add(Dropout(hp.Float('dropout_' + str(i), 0.4, 0.9, 0.1)))\n    \n    model.add(Dense(units=hp.Int('dense_units', 32, 256, 32), activation=hp.Choice('dense_activation', ['relu', 'tanh', 'sigmoid', 'elu'])))\n    model.add(Dropout(hp.Float('dense_dropout', 0.4, 0.9, 0.1)))\n    \n    model.add(Dense(units=1, activation=hp.Choice('dense_activation', ['relu', 'tanh', 'sigmoid', 'elu'])))\n    \n    # Epochs\n    num_epochs = hp.Int('num_epochs', min_value=10, max_value=100, step=10)\n    # batch_size\n    batch_size=hp.Int('batch_size', 16, 128, step=16)\n    \n    model.compile(optimizer=Adam(learning_rate=hp.Float('learning_rate', 0.0001, 0.01, step=0.0001)), \n                  loss='binary_crossentropy', \n                  metrics=['accuracy'])\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}